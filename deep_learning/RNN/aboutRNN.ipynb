{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "The basic idea behind RNNs is to make use of sequential information. In general neural networks, we assume that all inputs and outputs are independent. However, when we want to predict the next state under some sequential series, we better know the previous states. RNNs consider this with the output being depended on the previous computations. Another way to think about RNNs is that they have a **memory** which captures information about what has been calculated so far, but in practice they are limited to looking back only a few steps.\n",
    "\n",
    "<img src=\"./figs/rnns.png\">\n",
    "\n",
    "For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. \n",
    "\n",
    "The underlying computation of the above figure is as shown:\n",
    "* $x_t$ is the input at time step t.\n",
    "* $s_t$ is the hidden state at time stpe t. It is the **memory** of the network and it is calculated based on the previous hidden state and the input at the current time step as $s_t = f(Ux_t + Ws_{t-1})$. $f$ is usually nonlinearity such as tanh or ReLU. $s_{-1}$ is required at the first hidden state and typically initialized to all zeros.\n",
    "* $o_t$ is the output at time step t such as $o_t = softmax(Vs_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details:\n",
    "* $s_t$ captures information about what happened in all the previous time stpes and the output $o_t$ is calculated solely based on the memory at time step t. However, it is a bit more complicated in practice, because $s_t$ typically cannot capture information from too many time steps ago.\n",
    "\n",
    "* Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (U, V, W above) across all steps. This means that RNNs are performing the same task at each step, just with different inputs, which will greatly reduce the total  number of parameters we need to learn.\n",
    "\n",
    "* The above figure shows generated outputs at each time step, but this may not be necessary according to the given task. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
